# ğŸ¤– Makine Ã–ÄŸrenmesi EÄŸitim AlgoritmalarÄ± Ãœzerine NotlarÄ±m (Finansal)ğŸ“

Selamlar! Bu dÃ¶kÃ¼manda, projelerimde karÅŸÄ±laÅŸtÄ±ÄŸÄ±m ve kullandÄ±ÄŸÄ±m eÄŸitim algoritmalarÄ±nÄ±, net ve teknik bir dille, kendimden emin bir ÅŸekilde aÃ§Ä±klÄ±yorum. GitHub profilimde referans olarak kalacak.

---

## Tree Ensemble Methods ğŸŒ³ğŸŒ²

Bu bÃ¶lÃ¼mde, birden fazla karar aÄŸacÄ±nÄ± birleÅŸtirerek isabetli tahminler yapan topluluk yÃ¶ntemlerini ele alÄ±yoruz.

### 1. `FastForest`

* **Nedir?**: Bu, **Random Forest** (Rastgele Orman) algoritmasÄ±nÄ±n yÃ¼ksek performans iÃ§in optimize edilmiÅŸ bir uygulamasÄ±dÄ±r.
* **NasÄ±l Ã‡alÄ±ÅŸÄ±r?**: Ã‡ok sayÄ±da karar aÄŸacÄ±nÄ±, verinin ve Ã¶zniteliklerin rastgele alt kÃ¼meleriyle (*bagging* ve *feature bagging*) baÄŸÄ±msÄ±z olarak eÄŸitir. Tahminleri birleÅŸtirirken sÄ±nÄ±flandÄ±rma iÃ§in Ã§oÄŸunluk oylamasÄ±, regresyon iÃ§in ortalama alÄ±r.
* **"Fast" KÄ±smÄ±**: AdÄ±ndaki "Fast", arka planda kullanÄ±lan paralelleÅŸtirme, verimli bellek yÃ¶netimi ve hÄ±zlÄ± dÃ¼ÄŸÃ¼m bÃ¶lme (node splitting) gibi performans iyileÅŸtirmelerinden gelir.

### 2. `FastTree`

* **Nedir?**: Bu da hÄ±zlandÄ±rÄ±lmÄ±ÅŸ bir aÄŸaÃ§ topluluÄŸu algoritmasÄ±dÄ±r ve **Gradient Boosting** (Gradyan ArtÄ±rma) veya MART (Multiple Additive Regression Trees) mantÄ±ÄŸÄ± Ã¼zerine kuruludur.
* **NasÄ±l Ã‡alÄ±ÅŸÄ±r?**: AÄŸaÃ§lar ardÄ±ÅŸÄ±k olarak eÄŸitilir; her yeni aÄŸaÃ§, Ã¶nceki aÄŸaÃ§larÄ±n kolektif olarak yaptÄ±ÄŸÄ± hatalarÄ± (residual errors veya gradients) minimize etmeye odaklanÄ±r ve modeli adÄ±m adÄ±m gÃ¼Ã§lendirir.
* **"Fast" KÄ±smÄ±**: HÄ±zÄ±nÄ±, histogram tabanlÄ± bÃ¶lme (histogram-based splitting), verimli Ã¶nbellekleme (efficient caching) ve seyrek veri optimizasyonlarÄ± gibi modern tekniklere borÃ§ludur.

### 3. `LightGbm` (Light Gradient Boosting Machine)

* **Nedir?**: Gradient Boosting Decision Tree (GBDT) ailesinin en verimli ve popÃ¼ler Ã¼yelerinden biridir. YÃ¼ksek hÄ±zÄ± ve doÄŸruluÄŸu ile bilinir, Ã¶zellikle bÃ¼yÃ¼k veri setlerinde fark yaratÄ±r.
* **Anahtar Teknikleri**:
    * **GOSS (Gradient-based One-Side Sampling)**: BÃ¼yÃ¼k gradyanlÄ± (yani daha fazla hata iÃ§eren) Ã¶rneklere odaklanÄ±p, kÃ¼Ã§Ã¼k gradyanlÄ±larÄ± akÄ±llÄ±ca Ã¶rnekleyerek eÄŸitimi hÄ±zlandÄ±rÄ±r.
    * **EFB (Exclusive Feature Bundling)**: Birbirini dÄ±ÅŸlayan Ã¶znitelikleri (genellikle seyrek verilerde gÃ¶rÃ¼lÃ¼r) tek bir Ã¶znitelikmiÅŸ gibi ele alarak boyut azaltÄ±r ve hesaplama maliyetini dÃ¼ÅŸÃ¼rÃ¼r.
    * **Histogram TabanlÄ± Algoritmalar**: SÃ¼rekli Ã¶znitelikleri ayrÄ±k bÃ¶lmelere (bins) ayÄ±rarak en uygun bÃ¶lÃ¼nme noktasÄ±nÄ± bulma iÅŸlemini dramatik ÅŸekilde hÄ±zlandÄ±rÄ±r.
    * **Yaprak BazlÄ± BÃ¼yÃ¼me (Leaf-wise Growth)**: AÄŸacÄ± katman katman deÄŸil, kayÄ±pta en bÃ¼yÃ¼k azalmayÄ± saÄŸlayan yapraktan (leaf) bÃ¼yÃ¼terek daha optimize edilmiÅŸ aÄŸaÃ§lar oluÅŸturur.

---

## Linear Models & Optimizations ğŸ“âš™ï¸

DoÄŸrusal iliÅŸkileri modelleyen ve bunlarÄ± verimli ÅŸekilde optimize eden algoritmalara odaklanÄ±yoruz.

### 4. `AveragedPerceptron`

* **Nedir?**: Standart **Perceptron** algoritmasÄ±nÄ±n, kararlÄ±lÄ±ÄŸÄ± ve genelleme yeteneÄŸi artÄ±rÄ±lmÄ±ÅŸ bir varyasyonudur.
* **NasÄ±l Ã‡alÄ±ÅŸÄ±r?**: EÄŸitim boyunca her adÄ±mda (veya her gÃ¼ncellemede) oluÅŸan aÄŸÄ±rlÄ±k vektÃ¶rlerinin ortalamasÄ±nÄ± alarak nihai modeli oluÅŸturur. Bu ortalama iÅŸlemi, modelin veri setindeki gÃ¼rÃ¼ltÃ¼ye karÅŸÄ± daha dayanÄ±klÄ± olmasÄ±nÄ± saÄŸlar.

### 5. `LbfgsLogisticRegression`

* **Nedir?**: **Lojistik Regresyon** modelini (olasÄ±lÄ±ksal doÄŸrusal sÄ±nÄ±flandÄ±rÄ±cÄ±) eÄŸitmek iÃ§in **Limited-memory BFGS (L-BFGS)** optimizasyon algoritmasÄ±nÄ± kullanan bir yÃ¶ntemdir.
* **L-BFGS'in GÃ¼cÃ¼**: L-BFGS, ikinci tÃ¼rev bilgilerini (Hessian matrisi) doÄŸrudan hesaplamadan, gradyan geÃ§miÅŸini kullanarak bu bilgiyi yaklaÅŸÄ±k olarak tahmin eden bir quasi-Newton yÃ¶ntemidir. Bu sayede, Ã¶zellikle yÃ¼ksek boyutlu Ã¶znitelik uzaylarÄ±nda Lojistik Regresyon'u bellek ve hesaplama aÃ§Ä±sÄ±ndan Ã§ok verimli bir ÅŸekilde optimize eder.

### 6. `LdSvm` (Linear Dual Support Vector Machine)

* **Nedir?**: DoÄŸrusal bir Ã§ekirdek (kernel) kullanan **Destek VektÃ¶r Makinesi (SVM)** sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±dÄ±r ve optimizasyon iÃ§in SVM'nin **ikili (dual) problemini** Ã§Ã¶zer.
* **AvantajÄ±**: Ä°kili formÃ¼lasyon, Ã§Ã¶zÃ¼mÃ¼n yalnÄ±zca destek vektÃ¶rlerine baÄŸlÄ± olmasÄ± nedeniyle, Ã¶znitelik sayÄ±sÄ±nÄ±n veri noktasÄ± sayÄ±sÄ±ndan fazla olduÄŸu (yÃ¼ksek boyutlu, seyrek) durumlarda veya doÄŸrusal ayrÄ±mÄ±n yeterli olduÄŸu problemlerde birincil (primal) formÃ¼lasyona gÃ¶re daha verimli Ã§alÄ±ÅŸÄ±r.

### 7. `LinearSvm`

* **Nedir?**: DoÄŸrusal **Destek VektÃ¶r Makinesi (SVM)** sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±dÄ±r. AmacÄ±, farklÄ± sÄ±nÄ±flara ait veri noktalarÄ± arasÄ±ndaki **marjini (margin) maksimize eden** bir hiper-dÃ¼zlem (hyperplane) bulmaktÄ±r.
* **NasÄ±l Ã‡alÄ±ÅŸÄ±r?**: `Hinge loss` kayÄ±p fonksiyonunu (genellikle L1 veya L2 regÃ¼larizasyonu ile birlikte) minimize ederek bu hiper-dÃ¼zlemi bulur. DoÄŸrusal olarak ayrÄ±labilir veya buna yakÄ±n veri setleri ve yÃ¼ksek boyutlu Ã¶znitelik uzaylarÄ± iÃ§in etkili ve hÄ±zlÄ± bir Ã§Ã¶zÃ¼mdÃ¼r.

### 8. `SdcaLogisticRegression`

* **Nedir?**: Lojistik Regresyon modelini, **Stokastik Ä°kili Koordinat YÃ¼kseliÅŸi (SDCA)** optimizasyon algoritmasÄ± ile eÄŸitir.
* **SDCA'nÄ±n YaklaÅŸÄ±mÄ±**: Bu algoritma, Lojistik Regresyon'un (regÃ¼larizasyonlu) ikili (dual) optimizasyon problemini Ã§Ã¶zer. Her iterasyonda rastgele bir veri noktasÄ± seÃ§er ve bu noktaya karÅŸÄ±lÄ±k gelen ikili deÄŸiÅŸkeni (dual variable) optimize ederek ikili amaÃ§ fonksiyonunu maksimize eder. BÃ¼yÃ¼k Ã¶lÃ§ekli ve seyrek (sparse) veriler iÃ§in Ã§ok hÄ±zlÄ± yakÄ±nsama saÄŸlayan etkin bir yÃ¶ntemdir.

### 9. `SdcaNonCalibrated`

* **Nedir?**: SDCA kullanÄ±larak eÄŸitilmiÅŸ bir modeldir (Lojistik Regresyon veya DoÄŸrusal SVM), ancak Ã§Ä±ktÄ±sÄ± **kalibre edilmemiÅŸtir**.
* **AnlamÄ±**: Modelin Ã¼rettiÄŸi ham skorlar (Ã¶rneÄŸin, `w^T x + b` deÄŸeri) doÄŸrudan olasÄ±lÄ±k olarak yorumlanamaz. SÄ±ralama veya bir eÅŸik deÄŸere gÃ¶re karar verme iÃ§in kullanÄ±labilir, ancak gÃ¼venilir olasÄ±lÄ±k tahminleri iÃ§in ek bir kalibrasyon adÄ±mÄ±na (Platt scaling, isotonic regression vb.) ihtiyaÃ§ duyar.

### 10. `SgdCalibrated`

* **Nedir?**: Modeli (genellikle doÄŸrusal bir model) **Stokastik Gradyan Ä°niÅŸ (SGD)** ile eÄŸitir ve ardÄ±ndan Ã§Ä±ktÄ±larÄ±nÄ± **kalibre eder**.
* **SGD + Kalibrasyon**: SGD, her adÄ±mda kÃ¼Ã§Ã¼k veri gruplarÄ± (mini-batch) veya tekil Ã¶rnekler Ã¼zerinden gradyanÄ± hesaplayarak modeli gÃ¼ncelleyen, bÃ¼yÃ¼k veri setleri iÃ§in ideal bir optimizasyon algoritmasÄ±dÄ±r. "Calibrated" ifadesi, SGD ile elde edilen ham model Ã§Ä±ktÄ±larÄ±nÄ±n, daha doÄŸru olasÄ±lÄ±k tahminleri saÄŸlamak amacÄ±yla (Ã¶rneÄŸin Platt scaling veya isotonic regression ile) bir kalibrasyon iÅŸleminden geÃ§irildiÄŸini belirtir.

### 11. `SymbolicSgdLogisticRegression`

* **Nedir?**: SGD optimizasyonu kullanÄ±larak eÄŸitilen bir Lojistik Regresyon modelidir.
* **"Symbolic" KÄ±smÄ±**: Buradaki "Symbolic", gradyan hesaplamalarÄ±nÄ±n veya model yapÄ±sÄ±nÄ±n, sayÄ±sal optimizasyon sÄ±rasÄ±nda **sembolik matematik** araÃ§larÄ± kullanÄ±larak ifade edildiÄŸini veya iÅŸlendiÄŸini gÃ¶sterir. Bu, otomatik farklÄ±laÅŸma (automatic differentiation) saÄŸlar ve potansiyel derleme zamanÄ± optimizasyonlarÄ±na olanak tanÄ±r.

---

## Other Methods âœ¨

FarklÄ± yaklaÅŸÄ±mlara sahip diÄŸer algoritmalara da deÄŸinelim.

### 12. `FieldAwareFactorizationMachine` (FAFM)

* **Nedir?**: Bu, **FaktÃ¶rizasyon Makineleri**'nin (FM) **alan (field)** bilgisini dikkate alarak geliÅŸtirilmiÅŸ bir versiyonudur. Ã–zellikle yÃ¼ksek boyutlu ve seyrek kategorik verilerin (Ã¶rneÄŸin, online reklamcÄ±lÄ±kta tÄ±klama oranÄ± tahmini) modellenmesinde son derece etkilidir.
* **Ã–nemli FarkÄ±**: Standart FM, tÃ¼m Ã¶znitelik Ã§iftlerinin etkileÅŸimini tek tip bir gizli vektÃ¶r (latent vector) temsili Ã¼zerinden modeller. FAFM ise, her Ã¶zniteliÄŸin, etkileÅŸime girdiÄŸi diÄŸer Ã¶zniteliÄŸin **alanÄ±na (field)** Ã¶zgÃ¼ ayrÄ± bir gizli vektÃ¶rÃ¼ olmasÄ±nÄ± saÄŸlar (`v_{j,f_k}`). Bu sayede, Ã¶znitelikler arasÄ±ndaki etkileÅŸimleri (`<v_{j,f_k}, v_{k,f_j}>`) Ã§ok daha zengin ve baÄŸlama duyarlÄ± bir ÅŸekilde modeller.

### 13. `Gam` (Generalized Additive Model)

* **Nedir?**: GenelleÅŸtirilmiÅŸ DoÄŸrusal Modelleri (GLM), Ã¶zniteliklerin doÄŸrusal olmayan etkilerini yakalayacak ÅŸekilde geniÅŸleten yarÄ±-parametrik bir modeldir.
* **NasÄ±l Ã‡alÄ±ÅŸÄ±r?**: Hedef deÄŸiÅŸkenin beklenen deÄŸerini, bir baÄŸlantÄ± fonksiyonu (`g`) aracÄ±lÄ±ÄŸÄ±yla, Ã¶zniteliklerin **dÃ¼zgÃ¼n (smooth) fonksiyonlarÄ±nÄ±n (`s_i`)** toplamÄ±na baÄŸlar: `g(E[Y|X]) = \alpha + \sum_{i=1}^{p} s_i(X_i)`. Bu `s_i` fonksiyonlarÄ± (genellikle spline'lar kullanÄ±larak tahmin edilir) her bir Ã¶zniteliÄŸin hedef Ã¼zerindeki potansiyel eÄŸrisel etkisini esnek bir ÅŸekilde modellemeye olanak tanÄ±r. BÃ¶ylece hem doÄŸrusal olmayan iliÅŸkiler yakalanÄ±r hem de her Ã¶zniteliÄŸin katkÄ±sÄ± yorumlanabilir kalÄ±r.

### 14. `Prior`

* **Nedir?**: En temel **baseline** (referans noktasÄ±) modelidir. Girdi Ã¶zniteliklerini tamamen gÃ¶z ardÄ± eder.
* **Ne Yapar?**: Tahminlerini yalnÄ±zca eÄŸitim setindeki hedef deÄŸiÅŸkenin genel daÄŸÄ±lÄ±mÄ±na gÃ¶re yapar:
    * **SÄ±nÄ±flandÄ±rma**: Her zaman en sÄ±k gÃ¶rÃ¼len sÄ±nÄ±fÄ± (Ã§oÄŸunluk sÄ±nÄ±fÄ± - majority class) tahmin eder.
    * **Regresyon**: Her zaman hedef deÄŸiÅŸkenin ortalamasÄ±nÄ± veya medyanÄ±nÄ± tahmin eder.
* **AmacÄ±**: GeliÅŸtirilen daha karmaÅŸÄ±k modellerin gerÃ§ekten anlamlÄ± bir Ã¶ÄŸrenme gerÃ§ekleÅŸtirip gerÃ§ekleÅŸtirmediÄŸini Ã¶lÃ§mek iÃ§in bir alt sÄ±nÄ±r performansÄ± belirlemektir. Bir model `Prior`'dan daha iyi performans gÃ¶steremiyorsa, veriden deÄŸerli bir bilgi Ã§Ä±karamamÄ±ÅŸ demektir.

---